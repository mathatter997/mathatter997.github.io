<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="mathatter997.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="mathatter997.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-31T08:24:28+00:00</updated><id>mathatter997.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">P and NP</title><link href="mathatter997.github.io/blog/2024/np/" rel="alternate" type="text/html" title="P and NP"/><published>2024-07-28T00:32:13+00:00</published><updated>2024-07-28T00:32:13+00:00</updated><id>mathatter997.github.io/blog/2024/np</id><content type="html" xml:base="mathatter997.github.io/blog/2024/np/"><![CDATA[<p>Our goal is to provide a succinct and rigorous definition of NP. All of this discussion is based on the <a href="https://www.cl.cam.ac.uk/teaching/2324/L15/notes1.pdf" style="color:#87CEEB">first lecture</a> on logic and complexity by Anuj Dawar.</p> <h2> Background </h2> <p>First off, what are P and NP? They are sets of decisions problems defined by how long it takes a (non-)deterministic turing machine to solve them. P is the set of decision problems which can be solved in polynomial time using a turing machine. NP is the set of all problems which can be solved in polynomial time using a non-deterministinic turing machine. We elaborate more on turing machines in a bit. But for now, just think it as a machine loaded with a program which has a tape of symbols which it can read, write, and jump around on.</p> <figure> <p align="center"> <img title="Turing machine busy at work" src="/assets/img/turing-machine.png" height="300"/> </p> <figcaption> <p align="center"> <a href="https://craftinginterpreters.com/control-flow.html" style="color:#87CEEB">turing machine busy at work.</a> </p> </figcaption> </figure> <h3> Decision Problems </h3> <p>A decision problem is a yes-no question and it evaluates to true or false: deciding if \(\sqrt{2}\) is a rational number is a decision problem. Decision problems are fundementally different than counting problems which ask how many of something there is. For instance, given a rook at a1 on chess board, how many ways can it get h8 without crossing the main diagonal?</p> <p>Formally, we can define a decison problem as follows. Given a finite (set) alphabet \(\Sigma\), the set of all finite strings generated from over the alphabet is \(\Sigma^*\) and \(L \subseteq \Sigma^*\) is some subset of strings. We say \(L\) is decidable iff there is an algorithm, \(M\) (e.g turing machine), which correctly accepts or rejects all strings in \(\Sigma^*\) (i.e \(\forall x \in \Sigma^*\) \(M(x)\) terminates and \(M(x) =\) accept iff \(x \in L\)).</p> <h3> Turing Machines </h3> <p>Formally, a <em>deterministic</em> turing machine is has:</p> <ul> <li>\(K\): a finite set of states</li> <li>\(\Sigma\): a finite alphabet</li> <li>\(s\): an initial state</li> <li>\(\delta : (K \times \Sigma) \to K \cup \{acc, rej\} \times \Sigma \times \{L,S,R\}\): a transition function which takes as input the current state and symbol on the tape and then updates the state, writes (possible the same) symbol to the tape or terminates with an accept or reject. If the program hasn’t terminated, the machine also a step to the left or right along the tape, or remains stationary.</li> </ul> <p>A turing machine repeatedly applies the transition function, updating its current state and editing the tape. This process repeats forever or until the turing machine reaches a terminal state. Side note: deciding whether a turing machine will terminate for a given initialization is known as the halting problem and is <em>undecidable</em> in general.</p> <p>The only difference between a deterministic turing machine and a non-deterministic turing machine is that transition function is generalized to an arbitrary relation: \(\delta \subseteq (K \times \Sigma) \times (K \cup \{acc, rej\} \times \Sigma \times \{L,S,R\})\). In other words, each state and symbol can transition into multiple possible states. The power of non-determinism is that we can explore multiple branches of computation at once. A string \(x\) is accepted it at least on the branches produces an accept.</p> <figure> <p align="center"> <img title="Turing machine busy at work" src="/assets/img/p_vs_np.png" height="200"/> </p> <figcaption> <p align="center"> <a href="https://stackovercoder.fr/cs/80563/how-does-a-nondeterministic-turing-machine-work" style="color:#87CEEB">determinstic vs non-deterministic turing machines.</a> </p> </figcaption> </figure> <h2> Two definitions for NP </h2> <p>For a determinstic turing machine, the runtime is completely determined by current state \(q\), the tape upto and including the turing machine’s current positiuon \(w\), and the remaining tape \(u\). We call \((q, w, u)\) the configution and at the step the turing machine yields a new configuration according to the rules of the transition function:</p> \[(q, w, u) \to_M (q', w', u')\] <p>Define \(\to^*_M\) as the transitive and reflective closure of \(\to_M\). In other words, for a is a sequence of configurations \(c_1...c_n\) where \(c_i \to_M c_{i+1}\), then \(c_i \to^*_M c_j\) for all pairs \(i, j\) where \(i \leq j\). We call \(c_1...c_n\) a <em>computation</em> of \(M\).</p> <p>The language accepted by a turing machine \(L(M) \subseteq \Sigma^*\) is the set of strings:</p> \[L(M) := \{x|(s,\triangleright, x) \to^*_M (acc,w,u) \space \text{for some} \space w, u\}\] <p>For non-deteministic turing machines, the we define \(L(M)\) identically, although it’s important to note that \(\to_M\) and \(\to^*_M\) are no longer functional relationships.</p> <h3> Nondeterministic Complexity </h3> <p>For a function \(f: \mathbb{N} \to \mathbb{N}\), we say a language \(L\) is in \(NTIME(f(n))\) if there is a non-deterministic turing machine M, such that:</p> <ul> <li>\(L\) = \(L(M)\)</li> <li>\(\forall x \in L\) there is some accepting computation \(c_1...c_n\) such that \(n &lt; O(f(\\|x\\|))\).</li> </ul> <p>The last statement is a bit of a mouth-full, so we’ll just say “the running time of \(M\) is in \(O(f\\|x\\|)\)”. We now have all the tools we need to define NP:</p> \[NP := \bigcup_{k=1}^{\infty}NTIME(n^k)\] <p>We can define \(P\) and \(TIME(f(n))\) for deterministic turing machines similarily.</p> <h3> Succinct Certificates </h3> <p>Equivalenty, NP is the collect of languages \(L\) such that:</p> \[L = \{x | \exists y R(x,y)\}\] <p>where \(R\) is a relation on strings that is 1) decidable in polynomial time and 2) polynomially balanced: there is a polynomial \(p\) such that if \(R(x,y)\) and the length \(x\) is \(n\), then the length of \(y\) is no more than \(p(n)\).</p> <p>We’ll now show these definitions are equaivalent. Consider a language \(L = \{x \\| \exists y R(x,y)\}\). Using a non-deterministic machine M, we can use branching to guess at values for \(y\). Because \(R\) is polynomially-balanced, then there will be a computation polynominal in \(n=\\|x\\|\).</p> <p>Now suppose we are given a non-deterministic turing machine \(M\) which runs in time \(p(n)\). Because \(K\) and \(\Sigma\) are finite then there is some bound \(k\) for each state and symbol pair \((q, \sigma)\), \(\delta(q, \sigma)\) has at most \(k\) elements. Consider a string \(y\) over an alphabet \(\{1...k\}\). We define the relation \(R(x,y)\) by:</p> <ul> <li>\(\\|y\\| &lt; p(\\|x\\|)\),</li> <li>The computation of \(M\) on \(x\) which at step \(i\) takes the \(y[i]\)th transition is an accepting computation</li> </ul> <p>In other words, we can enumerate all possible computations \(x \in L\) with strings \(y\). Then by construction, \(R(x,y)\) iff there is an accepting computation of \(M\) of \(x\). What this means is that the language \(L' = \{x \\| \exists y R(x,y)\}\) is equivalent to \(L\). Done.</p> <p>Simply put, \(y\) represents any sequence of computations with length \(&lt; p(\\|x\\|)\). If \(R(x,y)\) then \(M(x)\) must accept \(x\). Referring to the image above, \(y\) is just an accepting branch of computation. It’s not too hard to see that we could construct a <em>determinstic</em> machine which takes in \(x\) and \(y\) as input and and can verify that \(R(x,y)\). This what is meant by saying that problems in NP are hard to solve but easy to verify: the lengths of their cerificates are polynomial so their verification can be done in P.</p>]]></content><author><name></name></author><category term="Complexity"/><category term="NP"/><category term="Complexity"/><category term="Turing-Machine"/><summary type="html"><![CDATA[Formal Description of NP]]></summary></entry><entry><title type="html">Gradient Flow with Minibatching</title><link href="mathatter997.github.io/blog/2024/simple-gf/" rel="alternate" type="text/html" title="Gradient Flow with Minibatching"/><published>2024-06-24T00:32:13+00:00</published><updated>2024-06-24T00:32:13+00:00</updated><id>mathatter997.github.io/blog/2024/simple-gf</id><content type="html" xml:base="mathatter997.github.io/blog/2024/simple-gf/"><![CDATA[<p>Gradient flow is gradient descent over continous time. Thinking about it another way, gradient descent is like applying a discrete ODE solver to a gradient flow problem. Given a set of parameters \(\theta \in \mathrm{R}^d\), we minimize an objective \(h : \mathrm{R}^d \to \mathrm{R}\) according to</p> \[\dot{\theta} = -\eta\nabla h(\theta).\] <p>Where \(\eta\) is the learning rate, which we take to be constant. Let’s consider a <em>very</em> simple example. Given a point \(\mathbf{a}\in\mathrm{R}^d\), we set our loss to be the squared error between \(\mathbf{a}\) and our parameters \(\theta\), \(h(\theta) := \frac{1}{2}\|\mathbf{a}-\theta\|^2\). Then the gradient flow becomes,</p> \[\dot{\theta} = \eta(\mathbf{a} - \theta).\] <p>Given \(\theta(0)=\boldsymbol{\theta}_0\), the solution is exactly \(\theta(t)=\mathbf{a} + (\boldsymbol{\theta}_0 - \mathbf{a})e^{-\eta t}\). Evidently, the dynamics follow a linear trajectory from \(\boldsymbol{\theta}_0\) to \(\mathbf{a}\).</p> <p align="center"> <img title="Linear Gradient Flow" src="/assets/img/lineargf.png"/> </p> <p>Let’s get slightly more complicated, let’s consider two datapoints: \(\mathbf{a},\mathbf{b}\). In full batch gradient descent, we compute the average loss w.r.t each datapoint <em>before</em> taking a gradient step. Perhaps unsurprisingly, this would result in a linear trajectory from \(\boldsymbol{\theta}_0\) to \(\frac{1}{2}(\mathbf{a} + \mathbf{b})\). In minibatch gradient descent, we break up our dataset in batches and compute the loss on a <em>per batch</em> basis. Typically, after a full pass through the data, we rebatch our dataset to prevent overfitting. In our case, we’ll only consider a round robin sampling between \(\mathbf{a}\) and \(\mathbf{b}\). We’ll model this as a toggeling between objectives using a square-wave with period \(2\eta\). The gradient flow is,</p> \[\begin{align*} \dot{\theta} &amp;= \eta\left[g_{2\eta}(t)(\mathbf{a} - \theta) + (1-g_{2\eta}(t))(\mathbf{b} - \theta)\right] \\ &amp;= \eta\left[g_{2\eta}(t)(\mathbf{a} - \mathbf{b}) + \mathbf{b} - \theta\right]. \end{align*}\] <p>where \(g_{2\eta}(t)\) is a square wave that alternates between 0 and 1 with period \(2\eta\). Note \(g_{2\eta}(t) = \frac{1}{2} + \frac{1}{2}s_{2\eta}(t)\), where \(s_{2\eta}(t)\) is a standard square wave that alternates between -1 and 1.</p> <p align="center"> <img title="Square wave" src="/assets/img/squarewave.png"/> </p> <p>We can solve the above equation using the Laplace transform,</p> \[\begin{align*} \mathcal{L}\{\dot{\theta}\} &amp;= \mathcal{L}\left\{\eta[\frac{\mathbf{a} + \mathbf{b}}{2} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t) - \theta]\right\} \\ \implies s\Theta(s) - \boldsymbol{\theta}_0 &amp;= \frac{\eta(\mathbf{a} + \mathbf{b})}{2s} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s}\tanh(\frac{\eta s}{2}) - \eta \Theta(s) {} \\ \implies (s + \eta)\Theta(s) &amp;= \boldsymbol{\theta}_0 + \frac{\eta(\mathbf{a} + \mathbf{b})}{2s} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s}\tanh(\frac{\eta s}{2}) \\ \implies \Theta(s) &amp;= \frac{\boldsymbol{\theta}_0}{s+\eta} + \frac{\eta(\mathbf{a} + \mathbf{b})}{2s(s+\eta)} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s(s+\eta)}\tanh(\frac{\eta s}{2}) \\ \implies \theta(t) &amp;= \frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}\mathcal{L}^{-1}\left\{\frac{\eta}{s(s+\eta)}\tanh(\frac{\eta s}{2})\right\}. \\ \end{align*}\] <p>Where used the fact that \(\mathcal{L}\{s_{2\eta}(t)\}=\frac{1}{s}\tanh{\frac{\eta s}{2}}\). We can further reduce the last term to,</p> \[\begin{align*} \mathcal{L}^{-1}\left\{\frac{\eta}{s(s+\eta)}\tanh(\frac{\eta s}{2})\right\} &amp;= \mathcal{L}^{-1}\left\{\left(\frac{1}{s} - \frac{1}{s+\eta}\right)\tanh(\frac{\eta s}{2})\right\} \\ &amp;= s_{2\eta}(t) - \mathcal{L}^{-1}\left\{\frac{1}{s+\eta}\tanh(\frac{\eta s}{2})\right\} \\ &amp;= s_{2\eta}(t) - \mathcal{L}^{-1}\left\{\frac{1}{s+\eta}\sum_{k=0}^{\infty}{e^{-2 k \eta s}-2e^{- (2k+1) \eta s}+e^{- 2(k+1) \eta s}}\right\} \\ &amp;= s_{2\eta}(t) - \sum_{k=0}^{\infty}\left[ e^{-\eta(t-2k\eta)}u(t-2k\eta) - 2e^{-\eta(t-(2k+1)\eta)}u(t-(2k+1)\eta) + e^{-\eta(t-2(k+1)\eta)}u(t-2(k+1)\eta) \right] \\ &amp;= s_{2\eta}(t) - e^{-\eta t}\left[\sum_{k=0}^{\infty}e^{2k\eta^2}u(t-2k\eta) - 2\sum_{k=0}^{\infty}e^{(2k+1)\eta^2}u(t-(2k+1)\eta) + \sum_{k=0}^{\infty}e^{2(k+1)\eta^2}u(t-2(k+1)\eta) \right] \\ &amp;= s_{2\eta}(t) - e^{-\eta t}\left[ \frac{1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)}}{1-e^{2\eta^2}} -2e^{\eta^2}\frac{1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}}{1-e^{2\eta^2}} + e^{2\eta^2}\frac{1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)}}{1-e^{2\eta^2}}\right] \\ &amp;= s_{2\eta}(t) - \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[ 1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} -2e^{\eta^2}(1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}) + e^{2\eta^2}(1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)})\right]. \\ \end{align*}\] <p>Where \(u(t)\) is the Heaviside function (0 for t &lt; 0, and 1 otherwise). And <em>Voilà</em> we’re done! Although this expression is somewhat unweildy. Let’s try to unpack what it’s saying. When \(0 \leq t&lt;\eta\), the <em>early</em> dynamics should exactly match the linear dynamics towards \(\mathbf{a}\). Indeed, the last two terms are exactly zero leaving \(s_{2\eta}(t) - e^{-\eta t}\), so the full expression simplifies to</p> \[\frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t) - \frac{\mathbf{a} - \mathbf{b}}{2}e^{-\eta t} = \mathbf{a} + (\boldsymbol{\theta}_0 - \mathbf{a})e^{-\eta t}.\] <p>In <em>terminal</em> dynamics, when \(t&gt;&gt;1\), the effect of the constant terms diminishes when multiplied by \(e^{-\eta t}\). We can simplify the term considerably:</p> \[\begin{align*} &amp; \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[ 1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} -2e^{\eta^2}(1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}) + e^{2\eta^2}(1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)})\right] \\ &amp;\approx \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} +2e^{\eta^2} \cdot e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)} -e^{2\eta^2} \cdot e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)}\right] \\ &amp;= -2\frac{(1-e^{-\eta^2 s_{2\eta}(t)})}{1-e^{2\eta^2}}e^{-\eta t + 2\eta^2 + 2\eta^2 \lfloor t/2\eta \rfloor}. \\ \end{align*}\] <p>Furthermore, if we also assume \(\eta &lt;&lt; 1\), then</p> \[\begin{align*} &amp; -2\frac{(1-e^{-\eta^2 s_{2\eta}(t)})}{1-e^{2\eta^2}}e^{-\eta t + 2\eta^2 + 2\eta^2 \lfloor t/2\eta \rfloor} \\ &amp;\approx 2\frac{\eta^2 s_{2\eta}(t)}{2\eta^2}e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor} \\ &amp;= s_{2\eta}(t)e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor}. \end{align*}\] <p>So we can approximate \(\theta(t)\) by,</p> \[\theta(t) \approx \frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t)(1-e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor}). \\\]]]></content><author><name></name></author><category term="ML-Theory"/><category term="math"/><category term="gradient-flow"/><summary type="html"><![CDATA[Gradient flow with a simple alternating objective.]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="mathatter997.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="mathatter997.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-24T20:18:48+00:00</updated><id>mathatter997.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Gradient Flow with Minibatching</title><link href="mathatter997.github.io/blog/2024/simple-gf/" rel="alternate" type="text/html" title="Gradient Flow with Minibatching"/><published>2024-06-24T00:32:13+00:00</published><updated>2024-06-24T00:32:13+00:00</updated><id>mathatter997.github.io/blog/2024/simple-gf</id><content type="html" xml:base="mathatter997.github.io/blog/2024/simple-gf/"><![CDATA[<p>Gradient flow is gradient descent over continous time. Thinking about it another way, gradient descent is like applying a discrete ODE solver to a gradient flow problem. Given a set of parameters \(\theta \in \mathrm{R}^d\), we minimize an objective \(h : \mathrm{R}^d \to \mathrm{R}\) according to</p> \[\dot{\theta} = -\eta\nabla h(\theta).\] <p>Where \(\eta\) is the learning rate, which we take to be constant. Let’s consider a <em>very</em> simple example. Given a point \(\mathbf{a}\in\mathrm{R}^d\), we set our loss to be the squared error between \(\mathbf{a}\) and our parameters \(\theta\), \(h(\theta) := \frac{1}{2}\|\mathbf{a}-\theta\|^2\). Then the gradient flow becomes,</p> \[\dot{\theta} = \eta(\mathbf{a} - \theta).\] <p>Given \(\theta(0)=\boldsymbol{\theta}_0\), the solution is exactly \(\theta(t)=\mathbf{a} + (\boldsymbol{\theta}_0 - \mathbf{a})e^{-\eta t}\). Evidently, the dynamics follow a linear trajectory from \(\boldsymbol{\theta}_0\) to \(\mathbf{a}\).</p> <p align="center"> <img title="Linear Gradient Flow" src="/assets/img/lineargf.png"/> </p> <p>Let’s get slightly more complicated, let’s consider two datapoints: \(\mathbf{a},\mathbf{b}\). In full batch gradient descent, we compute the average loss w.r.t each datapoint <em>before</em> taking a gradient step. Perhaps unsurprisingly, this would result in a linear trajectory from \(\boldsymbol{\theta}_0\) to \(\frac{1}{2}(\mathbf{a} + \mathbf{b})\). In minibatch gradient descent, we break up our dataset in batches and compute the loss on a <em>per batch</em> basis. Typically, after a full pass through the data, we rebatch our dataset to prevent overfitting. In our case, we’ll only consider a round robin sampling between \(\mathbf{a}\) and \(\mathbf{b}\). We’ll model this as a toggeling between objectives using a square-wave with period \(2\eta\). The gradient flow is,</p> \[\begin{align*} \dot{\theta} &amp;= \eta\left[g_{2\eta}(t)(\mathbf{a} - \theta) + (1-g_{2\eta}(t))(\mathbf{b} - \theta)\right] \\ &amp;= \eta\left[g_{2\eta}(t)(\mathbf{a} - \mathbf{b}) + \mathbf{b} - \theta\right]. \end{align*}\] <p>where \(g_{2\eta}(t)\) is a square wave that alternates between 0 and 1 with period \(2\eta\). Note \(g_{2\eta}(t) = \frac{1}{2} + \frac{1}{2}s_{2\eta}(t)\), where \(s_{2\eta}(t)\) is a standard square wave that alternates between -1 and 1.</p> <p align="center"> <img title="Square wave" src="/assets/img/squarewave.png"/> </p> <p>We can solve the above equation using the Laplace transform,</p> \[\begin{align*} \mathcal{L}\{\dot{\theta}\} &amp;= \mathcal{L}\left\{\eta[\frac{\mathbf{a} + \mathbf{b}}{2} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t) - \theta]\right\} \\ \implies s\Theta(s) - \boldsymbol{\theta}_0 &amp;= \frac{\eta(\mathbf{a} + \mathbf{b})}{2s} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s}\tanh(\frac{\eta s}{2}) - \eta \Theta(s) {} \\ \implies (s + \eta)\Theta(s) &amp;= \boldsymbol{\theta}_0 + \frac{\eta(\mathbf{a} + \mathbf{b})}{2s} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s}\tanh(\frac{\eta s}{2}) \\ \implies \Theta(s) &amp;= \frac{\boldsymbol{\theta}_0}{s+\eta} + \frac{\eta(\mathbf{a} + \mathbf{b})}{2s(s+\eta)} + \frac{\eta(\mathbf{a} - \mathbf{b})}{2s(s+\eta)}\tanh(\frac{\eta s}{2}) \\ \implies \theta(t) &amp;= \frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}\mathcal{L}^{-1}\left\{\frac{\eta}{s(s+\eta)}\tanh(\frac{\eta s}{2})\right\}. \\ \end{align*}\] <p>Where used the fact that \(\mathcal{L}\{s_{2\eta}(t)\}=\frac{1}{s}\tanh{\frac{\eta s}{2}}\). We can further reduce the last term to,</p> \[\begin{align*} \mathcal{L}^{-1}\left\{\frac{\eta}{s(s+\eta)}\tanh(\frac{\eta s}{2})\right\} &amp;= \mathcal{L}^{-1}\left\{\left(\frac{1}{s} - \frac{1}{s+\eta}\right)\tanh(\frac{\eta s}{2})\right\} \\ &amp;= s_{2\eta}(t) - \mathcal{L}^{-1}\left\{\frac{1}{s+\eta}\tanh(\frac{\eta s}{2})\right\} \\ &amp;= s_{2\eta}(t) - \mathcal{L}^{-1}\left\{\frac{1}{s+\eta}\sum_{k=0}^{\infty}{e^{-2 k \eta s}-2e^{- (2k+1) \eta s}+e^{- 2(k+1) \eta s}}\right\} \\ &amp;= s_{2\eta}(t) - \sum_{k=0}^{\infty}\left[ e^{-\eta(t-2k\eta)}u(t-2k\eta) - 2e^{-\eta(t-(2k+1)\eta)}u(t-(2k+1)\eta) + e^{-\eta(t-2(k+1)\eta)}u(t-2(k+1)\eta) \right] \\ &amp;= s_{2\eta}(t) - e^{-\eta t}\left[\sum_{k=0}^{\infty}e^{2k\eta^2}u(t-2k\eta) - 2\sum_{k=0}^{\infty}e^{(2k+1)\eta^2}u(t-(2k+1)\eta) + \sum_{k=0}^{\infty}e^{2(k+1)\eta^2}u(t-2(k+1)\eta) \right] \\ &amp;= s_{2\eta}(t) - e^{-\eta t}\left[ \frac{1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)}}{1-e^{2\eta^2}} -2e^{\eta^2}\frac{1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}}{1-e^{2\eta^2}} + e^{2\eta^2}\frac{1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)}}{1-e^{2\eta^2}}\right] \\ &amp;= s_{2\eta}(t) - \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[ 1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} -2e^{\eta^2}(1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}) + e^{2\eta^2}(1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)})\right]. \\ \end{align*}\] <p>Where \(u(t)\) is the Heaviside function (0 for t &lt; 0, and 1 otherwise). And <em>Voilà</em> we’re done! Although this expression is somewhat unweildy. Let’s try to unpack what it’s saying. When \(0 \leq t&lt;\eta\), the <em>early</em> dynamics should exactly match the linear dynamics towards \(\mathbf{a}\). Indeed, the last two terms are exactly zero leaving \(s_{2\eta}(t) - e^{-\eta t}\), so the full expression simplifies to</p> \[\frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t) - \frac{\mathbf{a} - \mathbf{b}}{2}e^{-\eta t} = \mathbf{a} + (\boldsymbol{\theta}_0 - \mathbf{a})e^{-\eta t}.\] <p>In <em>terminal</em> dynamics, when \(t&gt;&gt;1\), the effect of the constant terms diminishes when multiplied by \(e^{-\eta t}\). We can simplify the term considerably:</p> \[\begin{align*} &amp; \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[ 1-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} -2e^{\eta^2}(1-e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)}) + e^{2\eta^2}(1-e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)})\right] \\ &amp;\approx \frac{e^{-\eta t}}{1-e^{2\eta^2}}\left[-e^{2\eta^2(\lfloor t/2\eta \rfloor + 1)} +2e^{\eta^2} \cdot e^{2\eta^2(\lfloor (t-\eta)/2\eta \rfloor + 1)} -e^{2\eta^2} \cdot e^{2\eta^2(\lfloor (t-2\eta)/2\eta \rfloor + 1)}\right] \\ &amp;= -2\frac{(1-e^{-\eta^2 s_{2\eta}(t)})}{1-e^{2\eta^2}}e^{-\eta t + 2\eta^2 + 2\eta^2 \lfloor t/2\eta \rfloor}. \\ \end{align*}\] <p>Furthermore, if we also assume \(\eta &lt;&lt; 1\), then</p> \[\begin{align*} &amp; -2\frac{(1-e^{-\eta^2 s_{2\eta}(t)})}{1-e^{2\eta^2}}e^{-\eta t + 2\eta^2 + 2\eta^2 \lfloor t/2\eta \rfloor} \\ &amp;\approx 2\frac{\eta^2 s_{2\eta}(t)}{2\eta^2}e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor} \\ &amp;= s_{2\eta}(t)e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor}. \end{align*}\] <p>So we can approximate \(\theta(t)\) by,</p> \[\theta(t) \approx \frac{\mathbf{a} + \mathbf{b}}{2} + (\boldsymbol{\theta}_0 - \frac{\mathbf{a} + \mathbf{b}}{2})e^{-\eta t} + \frac{\mathbf{a} - \mathbf{b}}{2}s_{2\eta}(t)(1-e^{-\eta t + 2\eta^2 \lfloor t/2\eta \rfloor}). \\\]]]></content><author><name></name></author><category term="ML-Theory"/><category term="math"/><category term="gradient-flow"/><summary type="html"><![CDATA[Gradient flow with a simple alternating objective.]]></summary></entry></feed>